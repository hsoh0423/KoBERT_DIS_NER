{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 필요한 라이브러리 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "from KoBERT.kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from torch import nn\n",
    "from transformers import BertModel, BertConfig\n",
    "from torchcrf import CRF\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
    "from gluonnlp.data import BERTSPTokenizer\n",
    "import sentencepiece as spm\n",
    "from seqeval.metrics import f1_score\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, json_path):\n",
    "        with open(json_path, mode='r') as io:\n",
    "            params = json.loads(io.read())\n",
    "        self.__dict__.update(params)\n",
    "\n",
    "    def save(self, json_path):\n",
    "        with open(json_path, mode='w') as io:\n",
    "            json.dump(self.__dict__, io, indent=4)\n",
    "\n",
    "    def update(self, json_path):\n",
    "        with open(json_path, mode='r') as io:\n",
    "            params = json.loads(io.read())\n",
    "        self.__dict__.update(params)\n",
    "\n",
    "    @property\n",
    "    def dict(self):\n",
    "        return self.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KoBERT + CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#클래스 참고: KoBERT와 CRF로 만든 한국어 개체명 인식 https://github.com/eagle705/pytorch-bert-crf-ner\n",
    "\n",
    "bert_config = {'attention_probs_dropout_prob': 0.1,\n",
    "                 'hidden_act': 'gelu',\n",
    "                 'hidden_dropout_prob': 0.1,\n",
    "                 'hidden_size': 768,\n",
    "                 'initializer_range': 0.02,\n",
    "                 'intermediate_size': 3072,\n",
    "                 'max_position_embeddings': 512,\n",
    "                 'num_attention_heads': 12,\n",
    "                 'num_hidden_layers': 12,\n",
    "                 'type_vocab_size': 2,\n",
    "                 'vocab_size': 11575,\n",
    "                 'padding': True,\n",
    "                 'pair': False\n",
    "                 }\n",
    "                 \n",
    "class KobertCRF(nn.Module):\n",
    "    \"\"\" KoBERT with CRF \"\"\"\n",
    "    def __init__(self, config, num_classes, vocab=None) -> None:\n",
    "        super(KobertCRF, self).__init__()\n",
    "\n",
    "        self.bert = BertModel(config=BertConfig.from_dict(bert_config))\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.position_wise_ff = nn.Linear(config.hidden_size, num_classes)\n",
    "        self.crf = CRF(num_tags=num_classes, batch_first=True)\n",
    "\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "        \n",
    "    def forward(self, input_ids, valid_length, token_type_ids=None, tags=None):\n",
    "        attention_mask = self.gen_attention_mask(input_ids, valid_length)\n",
    "\n",
    "        # outputs: (last_encoder_layer, pooled_output, attention_weight)\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            attention_mask=attention_mask.float().to(input_ids.device))\n",
    "        last_encoder_layer = outputs[0]\n",
    "        last_encoder_layer = self.dropout(last_encoder_layer)\n",
    "        emissions = self.position_wise_ff(last_encoder_layer)\n",
    "\n",
    "        if tags is not None:\n",
    "            log_likelihood, sequence_of_tags = self.crf(emissions, tags), self.crf.decode(emissions)\n",
    "            return log_likelihood, sequence_of_tags\n",
    "        else:\n",
    "            sequence_of_tags = self.crf.decode(emissions)\n",
    "            return sequence_of_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer & Vocab 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer model 생성\n",
    "\n",
    "corpus = \"vocab.txt\" # KoCharELECTRA (https://github.com/monologg/KoCharELECTRA) vocab.txt 파일 사용 (음절 vocab 파일)\n",
    "prefix = \"vocab\"\n",
    "vocab_size = 11568 # vocab.txt 크기\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={corpus} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
    "    \" --model_type=bpe\" +\n",
    "    \" --max_sentence_length=10\" + # 문장 최대 길이\n",
    "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
    "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
    "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
    "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
    "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "bertmodel, _ = get_pytorch_kobert_model() # KoBERT 모델 불러오기\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece('vocab.model', padding_token='[PAD]')\n",
    "tokenizer = \"vocab.model\" #생성한 vocab.model 이용\n",
    "tok = BERTSPTokenizer(tokenizer, vocab, lower=False) # BERTSPTokenizer 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "p = re.compile('[一-龥]') #한자를 포함한 문장 제거하기 위한 정규식\n",
    "\n",
    "train_list_csv = pd.read_csv('Data/pet_train.csv', converters={\"tag\": literal_eval})\n",
    "test_list_csv = pd.read_csv('Data/pet_test.csv', converters={\"tag\": literal_eval})\n",
    "\n",
    "tr_tag = train_list_csv['tag']\n",
    "tr_sent = train_list_csv['sent']\n",
    "ts_tag = test_list_csv['tag']\n",
    "ts_sent = test_list_csv['sent']\n",
    "\n",
    "for i in range(len(train_list_csv['tag'])):\n",
    "    if p.search(tr_sent[i]): #한자 문장 제거\n",
    "        del tr_sent[i]\n",
    "        del tr_tag[i]\n",
    "    elif len(tr_tag[i]) != len(tr_sent[i])+3: #데이터 셋 준비 과정 중 길이 다르면 해당 인댁스 출력 +3 해주는 이유는 버트에 넣기 전 붙을 태그 등 데이터 떄문\n",
    "        print(i)\n",
    "    elif len(tr_tag[i]) > 512 or len(tr_sent[i])+3 > 512: # 문장의 길이가 512가 넘는 문장 제거\n",
    "        del tr_sent[i]\n",
    "        del tr_tag[i]\n",
    "\n",
    "for i in range(len(test_list_csv['tag'])):\n",
    "    if p.search(ts_sent[i]): #한자 문장 제거\n",
    "        del ts_sent[i]\n",
    "        del ts_tag[i]\n",
    "    elif 'Ⅳ' in ts_sent[i]: #로마자 제거\n",
    "        del ts_sent[i]\n",
    "        del ts_tag[i]\n",
    "    elif len(ts_tag[i]) != len(ts_sent[i])+3: #데이터 셋 준비 과정 중 길이 다르면 해당 인댁스 출력 +3 해주는 이유는 버트에 넣기 전 붙을 태그 등 데이터 떄문\n",
    "        print(i)\n",
    "    elif len(ts_tag[i]) > 512 or len(ts_sent[i])+3 > 512: # 문장의 길이가 512가 넘는 문장 제거\n",
    "        del ts_sent[i]\n",
    "        del ts_tag[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 버트 데이터 셋 클래스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 참고: 버트를 이용한 네이버 댓글 분류기 https://github.com/SKTBrain/KoBERT/blob/master/scripts/NSMC/naver_review_classifications_pytorch_kobert.ipynb\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, sent, tag, bert_tokenizer, max_len,\n",
    "                pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_len, pad=pad, pair=pair)\n",
    "        self.sentences = [transform([i]) for i in sent] #문장\n",
    "        self.labels = [np.pad(np.int32(i), (0,max_len-len(i)), 'constant', constant_values=0) for i in tag] #질병 태그 및 질병 태그 max_len 패딩\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용자 입력 문장을 테스하기 위한 데이터 셋 클래스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTTESTsent(Dataset):\n",
    "    def __init__(self, sent, bert_tokenizer, max_len,\n",
    "                pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_len, pad=pad, pair=pair)\n",
    "        self.sentences = [transform([i]) for i in sent] #문장\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i])\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 셋 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train = BERTDataset(tr_sent,tr_tag, tok, 512, True, False)\n",
    "data_test = BERTDataset(ts_sent,ts_tag, tok, 512, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=32, num_workers = 5, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=1, num_workers = 5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    5,   248,  7051,  7498,  8674,   974,  2098,  6882,  8031,\n",
       "         4131, 11251, 10998,   554,  7278,  7446,  7442,  4539, 11166,\n",
       "         8030,  8030,  6888,  6902,  8030,  3946, 11523,  4898,  7442,\n",
       "         1590,  7402,  6267,  7394,  5126,  3914,  4055,  7394,   603,\n",
       "         7246,   278,  7110,  2402,  7414,   935,   583, 11002,  4702,\n",
       "        11250,   278,   583,  3530, 11002,  7250,  2423,   278,  9458,\n",
       "         7446,  2542,  7022,  7442,  4314,  8038,  7442,   534,  3918,\n",
       "          603,  7246,   278,  3914,  2682,  7842,  7022,  7414, 11026,\n",
       "         4167,  7442,  7694,  7450,   603,  7246,   278,  9458,  7446,\n",
       "         2542,  7414,  7066,  7995,  7442,  1590,  9477,  4314,   278,\n",
       "        11250, 10999,  7610, 10790,  7442,  1590, 11075,  6070,  2703,\n",
       "        11254,   603,  7470,   919,  4314,  8038,   278,  4530,  7442,\n",
       "         3466,  6210,   278,  1249,   414,  3718,  6890,  3466,  8030,\n",
       "          278,  2194,  4558,  7054,   974,  2703,  7386,  3578,  9458,\n",
       "         5090,   435,  2990,  2098,  7446,  2542,   414,  7470,   919,\n",
       "         2486,  6994,   603,  3526,  7442,  7450,  6994,  1590,  4110,\n",
       "        11222, 11519,   642,  3358,  7386,  3578,  4538,  5755, 10998,\n",
       "         2098,   545,  7386,  3578,  6890,  3522,  7638,  7462,  6227,\n",
       "         2150,  2178,   280,     4,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       dtype=int32),\n",
       " array(166, dtype=int32),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " array([1, 4, 5, 6, 6, 6, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 6, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 6, 6, 6,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 5, 6, 4, 4, 4, 4, 4, 5, 6, 6, 6, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0], dtype=int32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_len(list):\n",
    "    count = 0\n",
    "    for i in range(len(list)):\n",
    "        if list[i] == 0:\n",
    "            break\n",
    "        else:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ae6381c0b64de6999ae8ef0b22e518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2754.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "data_tr_count = 0\n",
    "data_ts_count = 0\n",
    "for i in range(len(data_train)):\n",
    "    \n",
    "    if count_len(data_train[i][0]) != count_len(data_train[i][3]):\n",
    "        data_tr_count += 1\n",
    "        print(i)\n",
    "\n",
    "for i in tqdm_notebook(range(len(data_test))):\n",
    "\n",
    "    if count_len(data_test[i][0]) != count_len(data_test[i][3]):\n",
    "        data_ts_count += 1\n",
    "        print(i)\n",
    "print(data_tr_count, data_ts_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 선언 (초기 학습을 위한)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KobertCRF(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(11575, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (position_wise_ff): Linear(in_features=768, out_features=7, bias=True)\n",
       "  (crf): CRF(num_tags=7)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "model_config = Config('config.json')\n",
    "model = KobertCRF(config=model_config, num_classes=7)\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습된 모델 불러오기 (파인 튜닝을 위한)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('KoBERT_DIS_NER/Human_DIS(epoch1).pth')\n",
    "model_config = Config('config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(\n",
    "        nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "\n",
    "t_total = len(\n",
    "    train_dataloader) // model_config.gradient_accumulation_steps * model_config.epochs\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                    lr=model_config.learning_rate, eps=model_config.adam_epsilon)\n",
    "scheduler = WarmupLinearSchedule(\n",
    "    optimizer, warmup_steps=model_config.warmup_steps, t_total=t_total)\n",
    "\n",
    "model_dir = 'DIS_NER/'\n",
    "\n",
    "global_step = 0\n",
    "tr_loss, logging_loss = 0.0, 0.0\n",
    "model.zero_grad()\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/15 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad11b9c0491b4e8a86ca281dd1b06e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=492.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/RNTier/home/halee/.local/lib/python3.6/site-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, global_step : 1, tr_loss: 788.922, tr_acc: 90.15%\n",
      "epoch : 1, global_step : 101, tr_loss: 410.669, tr_acc: 96.01%\n",
      "epoch : 1, global_step : 201, tr_loss: 335.206, tr_acc: 95.14%\n",
      "epoch : 1, global_step : 301, tr_loss: 303.720, tr_acc: 96.27%\n",
      "epoch : 1, global_step : 401, tr_loss: 282.127, tr_acc: 98.81%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   7%|▋         | 1/15 [15:52<3:42:20, 952.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab5eb1b0e254cd3a875ce44645aeb3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=492.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 2, global_step : 493, tr_loss: 268.071, tr_acc: 98.05%\n",
      "epoch : 2, global_step : 593, tr_loss: 254.354, tr_acc: 98.13%\n",
      "epoch : 2, global_step : 693, tr_loss: 244.230, tr_acc: 98.72%\n",
      "epoch : 2, global_step : 793, tr_loss: 237.118, tr_acc: 99.40%\n",
      "epoch : 2, global_step : 893, tr_loss: 231.305, tr_acc: 98.85%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  13%|█▎        | 2/15 [31:46<3:26:29, 953.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3800e56084fa41e4b05bec262df018e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=492.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 3, global_step : 985, tr_loss: 227.594, tr_acc: 99.47%\n",
      "epoch : 3, global_step : 1085, tr_loss: 222.565, tr_acc: 99.80%\n",
      "epoch : 3, global_step : 1185, tr_loss: 218.564, tr_acc: 99.18%\n",
      "epoch : 3, global_step : 1285, tr_loss: 215.218, tr_acc: 99.61%\n",
      "epoch : 3, global_step : 1385, tr_loss: 212.437, tr_acc: 99.40%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|██        | 3/15 [47:40<3:10:39, 953.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c0818b75a8499ba9267d03f065fb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=492.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 4, global_step : 1477, tr_loss: 210.578, tr_acc: 99.17%\n",
      "epoch : 4, global_step : 1577, tr_loss: 208.279, tr_acc: 99.61%\n",
      "epoch : 4, global_step : 1677, tr_loss: 206.830, tr_acc: 99.25%\n",
      "epoch : 4, global_step : 1777, tr_loss: 206.209, tr_acc: 99.11%\n",
      "epoch : 4, global_step : 1877, tr_loss: 206.285, tr_acc: 99.65%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  27%|██▋       | 4/15 [1:03:43<2:55:20, 956.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e2f9b6ae8144259546deb81a1cf285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=492.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5, global_step : 1969, tr_loss: 207.190, tr_acc: 99.95%\n",
      "epoch : 5, global_step : 2069, tr_loss: 207.889, tr_acc: 99.37%\n",
      "epoch : 5, global_step : 2169, tr_loss: 209.280, tr_acc: 99.16%\n",
      "epoch : 5, global_step : 2269, tr_loss: 211.337, tr_acc: 99.88%\n",
      "epoch : 5, global_step : 2369, tr_loss: 213.298, tr_acc: 99.49%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|███▎      | 5/15 [1:19:48<2:39:50, 959.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00154ccdbb3947dba3a50a1685ab1c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=492.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 6, global_step : 2461, tr_loss: 215.248, tr_acc: 99.73%\n",
      "epoch : 6, global_step : 2561, tr_loss: 217.321, tr_acc: 99.66%\n",
      "epoch : 6, global_step : 2661, tr_loss: 219.722, tr_acc: 100.00%\n",
      "epoch : 6, global_step : 2761, tr_loss: 222.130, tr_acc: 99.70%\n",
      "epoch : 6, global_step : 2861, tr_loss: 224.555, tr_acc: 99.78%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|████      | 6/15 [1:35:51<2:23:59, 959.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56d359b97954bde8265a2060ef4ad75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=492.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 7, global_step : 2953, tr_loss: 226.549, tr_acc: 100.00%\n",
      "epoch : 7, global_step : 3053, tr_loss: 228.765, tr_acc: 99.94%\n",
      "epoch : 7, global_step : 3153, tr_loss: 231.052, tr_acc: 99.64%\n",
      "epoch : 7, global_step : 3253, tr_loss: 233.361, tr_acc: 99.64%\n",
      "epoch : 7, global_step : 3353, tr_loss: 235.791, tr_acc: 99.94%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  47%|████▋     | 7/15 [1:51:55<2:08:10, 961.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e89df27be804705925a733479875b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=492.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 8, global_step : 3445, tr_loss: 237.898, tr_acc: 99.70%\n",
      "epoch : 8, global_step : 3545, tr_loss: 240.166, tr_acc: 99.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  47%|████▋     | 7/15 [1:55:52<2:12:25, 993.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-c7a36cf4d8b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_of_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# loss: negative log-likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-58d9607bcb4d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, valid_length, token_type_ids, tags)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_of_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memissions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memissions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_of_tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchcrf/__init__.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, emissions, mask)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_viterbi_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memissions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     def _validate(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchcrf/__init__.py\u001b[0m in \u001b[0;36m_viterbi_decode\u001b[0;34m(self, emissions, mask)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;31m# sequence, and trace it back again, and so on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mseq_ends\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mbest_last_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mbest_tags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_last_tag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_iterator = trange(int(model_config.epochs), desc=\"Epoch\")\n",
    "device = torch.device('cuda')\n",
    "writer = SummaryWriter('scalar/')\n",
    "\n",
    "for _epoch, _ in enumerate(train_iterator):\n",
    "    epoch = _epoch\n",
    "    model.train()\n",
    "    for step, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    " \n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    " \n",
    "        log_likelihood, sequence_of_tags = model(token_ids,valid_length, segment_ids, label)\n",
    "\n",
    "        # loss: negative log-likelihood\n",
    "        loss = -1 * log_likelihood\n",
    "        writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "            \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), model_config.max_grad_norm)\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % model_config.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            with torch.no_grad():\n",
    "                sequence_of_tags = torch.tensor(\n",
    "                    sequence_of_tags).to(device)\n",
    "                mb_acc = (sequence_of_tags == label).float()[label != 0].mean()\n",
    "                writer.add_scalar(\"Acc/train\", mb_acc, epoch)\n",
    "\n",
    "            tr_acc = mb_acc.item()\n",
    "            tr_loss_avg = tr_loss / global_step\n",
    "            tr_summary = {'loss': tr_loss_avg, 'acc': tr_acc}\n",
    "            state = {'global_step': global_step + 1,\n",
    "                             'model_state_dict': model.state_dict(),\n",
    "                             'opt_state_dict': optimizer.state_dict()}\n",
    "            if step % 100 == 0:\n",
    "                print('epoch : {}, global_step : {}, tr_loss: {:.3f}, tr_acc: {:.2%}'.format(\n",
    "                        epoch + 1, global_step, tr_summary['loss'], tr_summary['acc']))\n",
    "                \n",
    "    torch.save(model, 'KoBERT_DIS_NER/Human_DIS(epoch'+str(epoch+1)+').pth')\n",
    "    print(\"save\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'KoBERT_DIS_NER/Human_PET_DIS.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_ner(sequences, val_len):\n",
    "\n",
    "    idx_to_ner = {\n",
    "        1 : \"[CLS]\", \n",
    "        2 : \"[SEP]\", \n",
    "        0 : \"O\", #[PAD] to 'O'\n",
    "        3 : \"[MASK]\",\n",
    "        4 : \"O\",\n",
    "        5 : \"B-DIS\",\n",
    "        6 : \"I-DIS\"\n",
    "    }\n",
    "    result = []\n",
    "    for i in range(val_len):\n",
    "        result.append(idx_to_ner[sequences[i]])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e006f7249c374741a35a2ce7c5a97bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2754.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_step : 10, evl_loss: 0.466, evl_acc: 1.00\n",
      "eval_step : 20, evl_loss: 0.240, evl_acc: 1.00\n",
      "eval_step : 30, evl_loss: 0.163, evl_acc: 1.00\n",
      "eval_step : 40, evl_loss: 0.122, evl_acc: 1.00\n",
      "eval_step : 50, evl_loss: 0.100, evl_acc: 1.00\n",
      "eval_step : 60, evl_loss: 0.083, evl_acc: 1.00\n",
      "eval_step : 70, evl_loss: 0.070, evl_acc: 1.00\n",
      "eval_step : 80, evl_loss: 0.062, evl_acc: 1.00\n",
      "eval_step : 90, evl_loss: 0.055, evl_acc: 1.00\n",
      "eval_step : 100, evl_loss: 0.109, evl_acc: 0.97\n",
      "eval_step : 110, evl_loss: 0.046, evl_acc: 1.00\n",
      "eval_step : 120, evl_loss: 0.040, evl_acc: 1.00\n",
      "eval_step : 130, evl_loss: 0.038, evl_acc: 1.00\n",
      "eval_step : 140, evl_loss: 0.089, evl_acc: 0.95\n",
      "eval_step : 150, evl_loss: 0.038, evl_acc: 1.00\n",
      "eval_step : 160, evl_loss: 0.033, evl_acc: 1.00\n",
      "eval_step : 170, evl_loss: 0.030, evl_acc: 1.00\n",
      "eval_step : 180, evl_loss: 0.028, evl_acc: 1.00\n",
      "eval_step : 190, evl_loss: 0.026, evl_acc: 1.00\n",
      "eval_step : 200, evl_loss: 0.027, evl_acc: 1.00\n",
      "eval_step : 210, evl_loss: 0.022, evl_acc: 1.00\n",
      "eval_step : 220, evl_loss: 0.021, evl_acc: 1.00\n",
      "eval_step : 230, evl_loss: 0.022, evl_acc: 1.00\n",
      "eval_step : 240, evl_loss: 0.020, evl_acc: 1.00\n",
      "eval_step : 250, evl_loss: 0.020, evl_acc: 1.00\n",
      "eval_step : 260, evl_loss: 0.019, evl_acc: 1.00\n",
      "eval_step : 270, evl_loss: 0.017, evl_acc: 1.00\n",
      "eval_step : 280, evl_loss: 0.032, evl_acc: 0.98\n",
      "eval_step : 290, evl_loss: 0.016, evl_acc: 1.00\n",
      "eval_step : 300, evl_loss: 0.017, evl_acc: 1.00\n",
      "eval_step : 310, evl_loss: 0.015, evl_acc: 1.00\n",
      "eval_step : 320, evl_loss: 0.015, evl_acc: 1.00\n",
      "eval_step : 330, evl_loss: 0.015, evl_acc: 1.00\n",
      "eval_step : 340, evl_loss: 0.015, evl_acc: 1.00\n",
      "eval_step : 350, evl_loss: 0.014, evl_acc: 1.00\n",
      "eval_step : 360, evl_loss: 0.013, evl_acc: 1.00\n",
      "eval_step : 370, evl_loss: 0.014, evl_acc: 1.00\n",
      "eval_step : 380, evl_loss: 0.013, evl_acc: 1.00\n",
      "eval_step : 390, evl_loss: 0.013, evl_acc: 1.00\n",
      "eval_step : 400, evl_loss: 0.012, evl_acc: 1.00\n",
      "eval_step : 410, evl_loss: 0.012, evl_acc: 1.00\n",
      "eval_step : 420, evl_loss: 0.012, evl_acc: 1.00\n",
      "eval_step : 430, evl_loss: 0.011, evl_acc: 1.00\n",
      "eval_step : 440, evl_loss: 0.011, evl_acc: 1.00\n",
      "eval_step : 450, evl_loss: 0.012, evl_acc: 1.00\n",
      "eval_step : 460, evl_loss: 0.010, evl_acc: 1.00\n",
      "eval_step : 470, evl_loss: 0.010, evl_acc: 1.00\n",
      "eval_step : 480, evl_loss: 0.010, evl_acc: 1.00\n",
      "eval_step : 490, evl_loss: 0.010, evl_acc: 1.00\n",
      "eval_step : 500, evl_loss: 0.010, evl_acc: 1.00\n",
      "eval_step : 510, evl_loss: 0.010, evl_acc: 1.00\n",
      "eval_step : 520, evl_loss: 0.010, evl_acc: 1.00\n",
      "eval_step : 530, evl_loss: 0.010, evl_acc: 1.00\n",
      "eval_step : 540, evl_loss: 0.039, evl_acc: 0.95\n",
      "eval_step : 550, evl_loss: 0.009, evl_acc: 1.00\n",
      "eval_step : 560, evl_loss: 0.009, evl_acc: 1.00\n",
      "eval_step : 570, evl_loss: 0.008, evl_acc: 1.00\n",
      "eval_step : 580, evl_loss: 0.008, evl_acc: 1.00\n",
      "eval_step : 590, evl_loss: 0.008, evl_acc: 1.00\n",
      "eval_step : 600, evl_loss: 0.009, evl_acc: 1.00\n",
      "eval_step : 610, evl_loss: 0.008, evl_acc: 1.00\n",
      "eval_step : 620, evl_loss: 0.011, evl_acc: 0.96\n",
      "eval_step : 630, evl_loss: 0.011, evl_acc: 0.98\n",
      "eval_step : 640, evl_loss: 0.008, evl_acc: 1.00\n",
      "eval_step : 650, evl_loss: 0.008, evl_acc: 1.00\n",
      "eval_step : 660, evl_loss: 0.014, evl_acc: 0.97\n",
      "eval_step : 670, evl_loss: 0.007, evl_acc: 1.00\n",
      "eval_step : 680, evl_loss: 0.007, evl_acc: 1.00\n",
      "eval_step : 690, evl_loss: 0.007, evl_acc: 1.00\n",
      "eval_step : 700, evl_loss: 0.007, evl_acc: 1.00\n",
      "eval_step : 710, evl_loss: 0.007, evl_acc: 1.00\n",
      "eval_step : 720, evl_loss: 0.007, evl_acc: 1.00\n",
      "eval_step : 730, evl_loss: 0.007, evl_acc: 1.00\n",
      "eval_step : 740, evl_loss: 0.007, evl_acc: 1.00\n",
      "eval_step : 750, evl_loss: 0.007, evl_acc: 1.00\n",
      "eval_step : 760, evl_loss: 0.006, evl_acc: 1.00\n",
      "eval_step : 770, evl_loss: 0.007, evl_acc: 1.00\n",
      "eval_step : 780, evl_loss: 0.006, evl_acc: 1.00\n",
      "eval_step : 790, evl_loss: 0.006, evl_acc: 1.00\n",
      "eval_step : 800, evl_loss: 0.007, evl_acc: 1.00\n",
      "eval_step : 810, evl_loss: 0.006, evl_acc: 1.00\n",
      "eval_step : 820, evl_loss: 0.007, evl_acc: 1.00\n",
      "eval_step : 830, evl_loss: 0.006, evl_acc: 1.00\n",
      "eval_step : 840, evl_loss: 0.006, evl_acc: 1.00\n",
      "eval_step : 850, evl_loss: 0.006, evl_acc: 1.00\n",
      "eval_step : 860, evl_loss: 0.006, evl_acc: 1.00\n",
      "eval_step : 870, evl_loss: 0.005, evl_acc: 1.00\n",
      "eval_step : 880, evl_loss: 0.005, evl_acc: 1.00\n",
      "eval_step : 890, evl_loss: 0.005, evl_acc: 1.00\n",
      "eval_step : 900, evl_loss: 0.005, evl_acc: 1.00\n",
      "eval_step : 910, evl_loss: 0.006, evl_acc: 1.00\n",
      "eval_step : 920, evl_loss: 0.005, evl_acc: 1.00\n",
      "eval_step : 930, evl_loss: 0.005, evl_acc: 1.00\n",
      "eval_step : 940, evl_loss: 0.005, evl_acc: 1.00\n",
      "eval_step : 950, evl_loss: 0.005, evl_acc: 1.00\n",
      "eval_step : 960, evl_loss: 0.005, evl_acc: 1.00\n",
      "eval_step : 970, evl_loss: 0.005, evl_acc: 1.00\n",
      "eval_step : 980, evl_loss: 0.005, evl_acc: 1.00\n",
      "eval_step : 990, evl_loss: 0.005, evl_acc: 1.00\n",
      "eval_step : 1000, evl_loss: 0.005, evl_acc: 1.00\n",
      "eval_step : 1010, evl_loss: 0.005, evl_acc: 1.00\n",
      "eval_step : 1020, evl_loss: 0.005, evl_acc: 1.00\n",
      "eval_step : 1030, evl_loss: 0.005, evl_acc: 1.00\n",
      "eval_step : 1040, evl_loss: 0.005, evl_acc: 1.00\n",
      "eval_step : 1050, evl_loss: 0.005, evl_acc: 1.00\n",
      "eval_step : 1060, evl_loss: 0.005, evl_acc: 1.00\n",
      "eval_step : 1070, evl_loss: 0.005, evl_acc: 1.00\n",
      "eval_step : 1080, evl_loss: 0.005, evl_acc: 1.00\n",
      "eval_step : 1090, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1100, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1110, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1120, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1130, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1140, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1150, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1160, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1170, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1180, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1190, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1200, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1210, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1220, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1230, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1240, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1250, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1260, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1270, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1280, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1290, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1300, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1310, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1320, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1330, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1340, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1350, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1360, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1370, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1380, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1390, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1400, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1410, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1420, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1430, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1440, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1450, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1460, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1470, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1480, evl_loss: 0.004, evl_acc: 0.97\n",
      "eval_step : 1490, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1500, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1510, evl_loss: 0.013, evl_acc: 0.97\n",
      "eval_step : 1520, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1530, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1540, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1550, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1560, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1570, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1580, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1590, evl_loss: 0.004, evl_acc: 1.00\n",
      "eval_step : 1600, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1610, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1620, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1630, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1640, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1650, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1660, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1670, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1680, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1690, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1700, evl_loss: 0.003, evl_acc: 1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_step : 1710, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1720, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1730, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1740, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1750, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1760, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1770, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1780, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1790, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1800, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1810, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1820, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1830, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1840, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1850, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1860, evl_loss: 0.004, evl_acc: 0.97\n",
      "eval_step : 1870, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 1880, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1890, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1900, evl_loss: 0.005, evl_acc: 0.99\n",
      "eval_step : 1910, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1920, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1930, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 1940, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 1950, evl_loss: 0.006, evl_acc: 0.94\n",
      "eval_step : 1960, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 1970, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 1980, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 1990, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2000, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2010, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2020, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2030, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2040, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 2050, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2060, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2070, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2080, evl_loss: 0.011, evl_acc: 0.95\n",
      "eval_step : 2090, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2100, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2110, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 2120, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2130, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2140, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2150, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2160, evl_loss: 0.005, evl_acc: 0.96\n",
      "eval_step : 2170, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2180, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2190, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 2200, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2210, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2220, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2230, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2240, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2250, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2260, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2270, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2280, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2290, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2300, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2310, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2320, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2330, evl_loss: 0.004, evl_acc: 0.98\n",
      "eval_step : 2340, evl_loss: 0.004, evl_acc: 0.97\n",
      "eval_step : 2350, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2360, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2370, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2380, evl_loss: 0.005, evl_acc: 0.95\n",
      "eval_step : 2390, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2400, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2410, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2420, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2430, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2440, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2450, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2460, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2470, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2480, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2490, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2500, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2510, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2520, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2530, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2540, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2550, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2560, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2570, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2580, evl_loss: 0.003, evl_acc: 1.00\n",
      "eval_step : 2590, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2600, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2610, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2620, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2630, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2640, evl_loss: 0.005, evl_acc: 0.95\n",
      "eval_step : 2650, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2660, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2670, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2680, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2690, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2700, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2710, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2720, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2730, evl_loss: 0.002, evl_acc: 1.00\n",
      "eval_step : 2740, evl_loss: 0.002, evl_acc: 0.92\n",
      "eval_step : 2750, evl_loss: 0.002, evl_acc: 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "model_config = Config('config.json')\n",
    "model = torch.load('KoBERT_DIS_NER/Human_DIS(epoch4).pth')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "eval_step = 0\n",
    "sum_f1 = 0\n",
    "predic_list = []\n",
    "true_list = []\n",
    "predic_list2= []\n",
    "true_list2 = []\n",
    "for step, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "    \n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    label = label.long().to(device)\n",
    "\n",
    "    log_likelihood, sequence_of_tags = model(token_ids,valid_length, segment_ids, label)\n",
    "    # loss: negative log-likelihood\n",
    "    eval_loss = -1 * log_likelihood\n",
    "\n",
    "    if (eval_step + 1) % model_config.gradient_accumulation_steps == 0:\n",
    "        eval_step += 1\n",
    "        with torch.no_grad():\n",
    "            sequence_of_tags = torch.tensor(\n",
    "                sequence_of_tags).to(device)\n",
    "            eval_acc = (sequence_of_tags == label).float()[label != 0].mean()\n",
    "        eval_loss_avg = eval_loss / eval_step\n",
    "        eval_summary = {'loss': eval_loss_avg, 'acc': eval_acc}\n",
    "\n",
    "        predic_list.append(index_to_ner(sequence_of_tags[0].cpu().numpy(), valid_length[0].cpu().numpy()))\n",
    "        true_list.append(index_to_ner(label[0].cpu().numpy(), valid_length[0].cpu().numpy()))\n",
    "        \n",
    "        if eval_step % 10 == 0:\n",
    "            print('eval_step : {}, evl_loss: {:.3f}, evl_acc: {:.2f}'.format(\n",
    "                eval_step, eval_summary['loss'], eval_summary['acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 스코어 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9843581445523193"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(true_list, predic_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용자 입력 문장 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertmodel, _ = get_pytorch_kobert_model() # KoBERT 모델 불러오기\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece('vocab.model', padding_token='[PAD]')\n",
    "tokenizer = \"vocab.model\" #생성한 vocab.model 이용\n",
    "tok = BERTSPTokenizer(tokenizer, vocab, lower=False) # BERTSPTokenizer 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### model_config = Config('config.json')\n",
    "\n",
    "# model\n",
    "model = torch.load('KoBERT_DIS_NER/Human_PET_DIS.pth')\n",
    "model.eval()\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "input_text = input('input> ')\n",
    "print(input_text)\n",
    "text_sent = []\n",
    "text_sent.append(input_text)\n",
    "sent_test = BERTTESTsent(text_sent, tok, 512, True, False)\n",
    "sent_dataloader = torch.utils.data.DataLoader(sent_test, batch_size=1, num_workers = 5)\n",
    "\n",
    "for _, (token_ids, valid_length, segment_ids) in enumerate(tqdm_notebook(sent_dataloader)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids =  segment_ids.long().to(device)\n",
    "    valid_length =  valid_length\n",
    "    sequence_of_tags = model(token_ids, valid_length, segment_ids)\n",
    "    print(sequence_of_tags)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7999ffa372d5518938264c39b302fc0f2ea2294124935d58b3105141df28eea9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
