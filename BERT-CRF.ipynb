{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 필요한 라이브러리 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "from KoBERT.kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from torch import nn\n",
    "from transformers import BertModel, BertConfig\n",
    "from torchcrf import CRF\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
    "from gluonnlp.data import BERTSPTokenizer\n",
    "import sentencepiece as spm\n",
    "from seqeval.metrics import f1_score\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, json_path):\n",
    "        with open(json_path, mode='r') as io:\n",
    "            params = json.loads(io.read())\n",
    "        self.__dict__.update(params)\n",
    "\n",
    "    def save(self, json_path):\n",
    "        with open(json_path, mode='w') as io:\n",
    "            json.dump(self.__dict__, io, indent=4)\n",
    "\n",
    "    def update(self, json_path):\n",
    "        with open(json_path, mode='r') as io:\n",
    "            params = json.loads(io.read())\n",
    "        self.__dict__.update(params)\n",
    "\n",
    "    @property\n",
    "    def dict(self):\n",
    "        return self.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KoBERT + CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#클래스 참고: KoBERT와 CRF로 만든 한국어 개체명 인식 https://github.com/eagle705/pytorch-bert-crf-ner\n",
    "\n",
    "bert_config = {'attention_probs_dropout_prob': 0.1,\n",
    "                 'hidden_act': 'gelu',\n",
    "                 'hidden_dropout_prob': 0.1,\n",
    "                 'hidden_size': 768,\n",
    "                 'initializer_range': 0.02,\n",
    "                 'intermediate_size': 3072,\n",
    "                 'max_position_embeddings': 512,\n",
    "                 'num_attention_heads': 12,\n",
    "                 'num_hidden_layers': 12,\n",
    "                 'type_vocab_size': 2,\n",
    "                 'vocab_size': 11575,\n",
    "                 'padding': True,\n",
    "                 'pair': False\n",
    "                 }\n",
    "                 \n",
    "class KobertCRF(nn.Module):\n",
    "    \"\"\" KoBERT with CRF \"\"\"\n",
    "    def __init__(self, config, num_classes, vocab=None) -> None:\n",
    "        super(KobertCRF, self).__init__()\n",
    "\n",
    "        self.bert = BertModel(config=BertConfig.from_dict(bert_config))\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.position_wise_ff = nn.Linear(config.hidden_size, num_classes)\n",
    "        self.crf = CRF(num_tags=num_classes, batch_first=True)\n",
    "\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "        \n",
    "    def forward(self, input_ids, valid_length, token_type_ids=None, tags=None):\n",
    "        attention_mask = self.gen_attention_mask(input_ids, valid_length)\n",
    "\n",
    "        # outputs: (last_encoder_layer, pooled_output, attention_weight)\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            attention_mask=attention_mask.float().to(input_ids.device))\n",
    "        last_encoder_layer = outputs[0]\n",
    "        last_encoder_layer = self.dropout(last_encoder_layer)\n",
    "        emissions = self.position_wise_ff(last_encoder_layer)\n",
    "\n",
    "        if tags is not None:\n",
    "            log_likelihood, sequence_of_tags = self.crf(emissions, tags), self.crf.decode(emissions)\n",
    "            return log_likelihood, sequence_of_tags\n",
    "        else:\n",
    "            sequence_of_tags = self.crf.decode(emissions)\n",
    "            return sequence_of_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer & Vocab 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer model 생성\n",
    "\n",
    "corpus = \"vocab.txt\" # KoCharELECTRA (https://github.com/monologg/KoCharELECTRA) vocab.txt 파일 사용 (음절 vocab 파일)\n",
    "prefix = \"vocab\"\n",
    "vocab_size = 11568 # vocab.txt 크기\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={corpus} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
    "    \" --model_type=bpe\" +\n",
    "    \" --max_sentence_length=10\" + # 문장 최대 길이\n",
    "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
    "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
    "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
    "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
    "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece('vocab.model', padding_token='[PAD]')\n",
    "tokenizer = \"vocab.model\" #생성한 vocab.model 이용\n",
    "tok = BERTSPTokenizer(tokenizer, vocab, lower=False) # BERTSPTokenizer 생성|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "p = re.compile('[一-龥]') #한자를 포함한 문장 제거하기 위한 정규식\n",
    "\n",
    "train_list_csv = pd.read_csv('Data/pet_train_V3.csv', converters={\"tag\": literal_eval})\n",
    "test_list_csv = pd.read_csv('Data/pet_test_V3.csv', converters={\"tag\": literal_eval})\n",
    "\n",
    "tr_tag = train_list_csv['tag']\n",
    "tr_sent = train_list_csv['sent']\n",
    "ts_tag = test_list_csv['tag']\n",
    "ts_sent = test_list_csv['sent']\n",
    "\n",
    "for i in range(len(train_list_csv['tag'])):\n",
    "    if p.search(tr_sent[i]): #한자 문장 제거\n",
    "        del tr_sent[i]\n",
    "        del tr_tag[i]\n",
    "    elif len(tr_tag[i]) != len(tr_sent[i])+3: #데이터 셋 준비 과정 중 길이 다르면 해당 인댁스 출력 +3 해주는 이유는 버트에 넣기 전 붙을 태그 등 데이터 떄문\n",
    "        print(i)\n",
    "    elif len(tr_tag[i]) > 512 or len(tr_sent[i])+3 > 512: # 문장의 길이가 512가 넘는 문장 제거\n",
    "        del tr_sent[i]\n",
    "        del tr_tag[i]\n",
    "\n",
    "for i in range(len(test_list_csv['tag'])):\n",
    "    if p.search(ts_sent[i]): #한자 문장 제거\n",
    "        del ts_sent[i]\n",
    "        del ts_tag[i]\n",
    "    elif 'Ⅳ' in ts_sent[i]: #로마자 제거\n",
    "        del ts_sent[i]\n",
    "        del ts_tag[i]\n",
    "    elif len(ts_tag[i]) != len(ts_sent[i])+3: #데이터 셋 준비 과정 중 길이 다르면 해당 인댁스 출력 +3 해주는 이유는 버트에 넣기 전 붙을 태그 등 데이터 떄문\n",
    "        print(i)\n",
    "    elif len(ts_tag[i]) > 512 or len(ts_sent[i])+3 > 512: # 문장의 길이가 512가 넘는 문장 제거\n",
    "        del ts_sent[i]\n",
    "        del ts_tag[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 버트 데이터 셋 클래스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 참고: 버트를 이용한 네이버 댓글 분류기 https://github.com/SKTBrain/KoBERT/blob/master/scripts/NSMC/naver_review_classifications_pytorch_kobert.ipynb\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, sent, tag, bert_tokenizer, max_len,\n",
    "                pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_len, pad=pad, pair=pair)\n",
    "        self.sentences = [transform([i]) for i in sent] #문장\n",
    "        self.labels = [np.pad(np.int32(i), (0,max_len-len(i)), 'constant', constant_values=0) for i in tag] #질병 태그 및 질병 태그 max_len 패딩\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용자 입력 문장을 테스하기 위한 데이터 셋 클래스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTTESTsent(Dataset):\n",
    "    def __init__(self, sent, bert_tokenizer, max_len,\n",
    "                pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_len, pad=pad, pair=pair)\n",
    "        self.sentences = [transform([i]) for i in sent] #문장\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i])\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 셋 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train = BERTDataset(tr_sent,tr_tag, tok, 512, True, False)\n",
    "data_test = BERTDataset(ts_sent,ts_tag, tok, 512, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=32, num_workers = 5, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=1, num_workers = 5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_len(list):\n",
    "    count = 0\n",
    "    for i in range(len(list)):\n",
    "        if list[i] == 0:\n",
    "            break\n",
    "        else:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr_count = 0\n",
    "data_ts_count = 0\n",
    "for i in range(len(data_train)):\n",
    "    \n",
    "    if count_len(data_train[i][0]) != count_len(data_train[i][3]):\n",
    "        data_tr_count += 1\n",
    "        print(i)\n",
    "\n",
    "for i in tqdm_notebook(range(len(data_test))):\n",
    "\n",
    "    if count_len(data_test[i][0]) != count_len(data_test[i][3]):\n",
    "        data_ts_count += 1\n",
    "        print(i)\n",
    "print(data_tr_count, data_ts_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 선언 (초기 학습을 위한)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KobertCRF(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(11575, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (position_wise_ff): Linear(in_features=768, out_features=7, bias=True)\n",
       "  (crf): CRF(num_tags=7)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "model_config = Config('config.json')\n",
    "model = KobertCRF(config=model_config, num_classes=7)\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습된 모델 불러오기 (파인 튜닝을 위한)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('KoBERT_DIS_NER_V3/Human_DIS_V3(epoch2, step1100).pth')\n",
    "model_config = Config('config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(\n",
    "        nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "\n",
    "t_total = len(\n",
    "    train_dataloader) // model_config.gradient_accumulation_steps * model_config.epochs\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                    lr=model_config.learning_rate, eps=model_config.adam_epsilon)\n",
    "scheduler = WarmupLinearSchedule(\n",
    "    optimizer, warmup_steps=model_config.warmup_steps, t_total=t_total)\n",
    "\n",
    "model_dir = 'DIS_NER/'\n",
    "\n",
    "global_step = 0\n",
    "tr_loss, logging_loss = 0.0, 0.0\n",
    "model.zero_grad()\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/150 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b885be077644cd78eb517aa4c76230d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=476.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/RNTier/home/halee/.local/lib/python3.6/site-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, global_step : 1, tr_loss: 31921.584, tr_acc: 7.61%\n",
      "torch.Size([32, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/150 [00:05<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-caebd26cbe71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         torch.nn.utils.clip_grad_norm_(\n\u001b[1;32m     22\u001b[0m             model.parameters(), model_config.max_grad_norm)\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_iterator = trange(int(model_config.epochs), desc=\"Epoch\")\n",
    "device = torch.device('cuda')\n",
    "\n",
    "for _epoch, _ in enumerate(train_iterator):\n",
    "    epoch = _epoch\n",
    "    model.train()\n",
    "    for step, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        log_likelihood, sequence_of_tags = model(token_ids,valid_length, segment_ids, label)\n",
    "        \n",
    "        # loss: negative log-likelihood\n",
    "        loss = -1 * log_likelihood\n",
    "            \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), model_config.max_grad_norm)\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % model_config.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            with torch.no_grad():\n",
    "                sequence_of_tags = torch.tensor(\n",
    "                    sequence_of_tags).to(device)\n",
    "                mb_acc = (sequence_of_tags == label).float()[label != 0].mean()\n",
    "\n",
    "            tr_acc = mb_acc.item()\n",
    "            tr_loss_avg = tr_loss / global_step\n",
    "            tr_summary = {'loss': tr_loss_avg, 'acc': tr_acc}\n",
    "            state = {'global_step': global_step + 1,\n",
    "                             'model_state_dict': model.state_dict(),\n",
    "                             'opt_state_dict': optimizer.state_dict()}\n",
    "            if step % 100 == 0:\n",
    "                print('epoch : {}, global_step : {}, tr_loss: {:.3f}, tr_acc: {:.2%}'.format(\n",
    "                        epoch + 1, global_step, tr_summary['loss'], tr_summary['acc']))  \n",
    "                torch.save(model, 'KoBERT_DIS_NER_V3/Human_DIS_V4(epoch'+str(epoch+1)+\", step\"+str(step)+').pth')\n",
    "    print(\"epoch finish\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_ner(sequences, val_len):\n",
    "\n",
    "    idx_to_ner = {\n",
    "        1 : \"[CLS]\", \n",
    "        2 : \"[SEP]\", \n",
    "        0 : \"O\", #[PAD] to 'O'\n",
    "        3 : \"[MASK]\",\n",
    "        4 : \"O\",\n",
    "        5 : \"B-DIS\",\n",
    "        6 : \"I-DIS\"\n",
    "    }\n",
    "    result = []\n",
    "    for i in range(val_len):\n",
    "        result.append(idx_to_ner[sequences[i]])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model_config = Config('config.json')\n",
    "model = torch.load('Human_Pet_DIS_V3(epoch3, step300).pth')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "vocab_file = \"vocab.model\"\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(vocab_file)\n",
    "eval_step = 0\n",
    "sum_f1 = 0\n",
    "predic_list = []\n",
    "true_list = []\n",
    "dis_list2 = []\n",
    "for step, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "    \n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    label = label.long().to(device)\n",
    "\n",
    "    log_likelihood, sequence_of_tags = model(token_ids,valid_length, segment_ids, label)\n",
    "    # loss: negative log-likelihood\n",
    "    eval_loss = -1 * log_likelihood\n",
    "    \n",
    "    temp_sent = ''\n",
    "    \n",
    "    for i in range(int(valid_length)):\n",
    "        if sequence_of_tags[0][i] == 5 or sequence_of_tags[0][i] == 6:\n",
    "            if sequence_of_tags[0][i] == 5 and len(temp_sent) > 0:\n",
    "                dis_list2.append(temp_sent)\n",
    "                temp_sent = vocab.IdToPiece(int(token_ids[0][i]))\n",
    "            else:\n",
    "                temp_sent += vocab.IdToPiece(int(token_ids[0][i]))\n",
    "        elif len(temp_sent) > 0:\n",
    "            dis_list2.append(temp_sent)\n",
    "            temp_sent = ''\n",
    "            \n",
    "    if (eval_step + 1) % model_config.gradient_accumulation_steps == 0:\n",
    "        eval_step += 1\n",
    "        with torch.no_grad():\n",
    "            sequence_of_tags = torch.tensor(\n",
    "                sequence_of_tags).to(device)\n",
    "            eval_acc = (sequence_of_tags == label).float()[label != 0].mean()\n",
    "        eval_loss_avg = eval_loss / eval_step\n",
    "        eval_summary = {'loss': eval_loss_avg, 'acc': eval_acc}\n",
    "\n",
    "        predic_list.append(index_to_ner(sequence_of_tags[0].cpu().numpy(), valid_length[0].cpu().numpy()))\n",
    "        true_list.append(index_to_ner(label[0].cpu().numpy(), valid_length[0].cpu().numpy()))\n",
    "        \n",
    "        if eval_step % 10 == 0:\n",
    "            print('eval_step : {}, evl_loss: {:.3f}, evl_acc: {:.2f}'.format(\n",
    "                eval_step, eval_summary['loss'], eval_summary['acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 스코어 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f1_score(true_list, predic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_list2 = list(set(dis_list2))\n",
    "f = open(\"humanModel_pet_test_dis_list_V3.txt\", 'w')\n",
    "\n",
    "for line in dis_list2:\n",
    "    f.write(line+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용자 입력 문장 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece('vocab.model', padding_token='[PAD]')\n",
    "tokenizer = \"vocab.model\" #생성한 vocab.model 이용\n",
    "tok = BERTSPTokenizer(tokenizer, vocab, lower=False) # BERTSPTokenizer 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input> 병원에서 정확한 진단과 적절한 처치 받아보시길 권해드립니다.\n",
      "병원에서 정확한 진단과 적절한 처치 받아보시길 권해드립니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deec8ec2901f49b88e98ce4da50a21dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['▁진']\n"
     ]
    }
   ],
   "source": [
    "model_config = Config('config.json')\n",
    "\n",
    "# model\n",
    "model = torch.load('KoBERT_DIS_NER_V3/Human_Pet_DIS_V3(epoch3, step300).pth')\n",
    "model.eval()\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "vocab_file = \"vocab.model\"\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(vocab_file)\n",
    "input_text = input('input> ')\n",
    "print(input_text)\n",
    "text_sent = []\n",
    "text_sent.append(input_text)\n",
    "sent_test = BERTTESTsent(text_sent, tok, 512, True, False)\n",
    "sent_dataloader = torch.utils.data.DataLoader(sent_test, batch_size=1, num_workers = 5)\n",
    "temp_sent = ''\n",
    "dis_list = []\n",
    "\n",
    "for _, (token_ids, valid_length, segment_ids) in enumerate(tqdm_notebook(sent_dataloader)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids =  segment_ids.long().to(device)\n",
    "    valid_length =  valid_length\n",
    "    sequence_of_tags = model(token_ids, valid_length, segment_ids)\n",
    "    \n",
    "    for i in range(int(valid_length)):\n",
    "        if sequence_of_tags[0][i] == 5 or sequence_of_tags[0][i] == 6:\n",
    "            if sequence_of_tags[0][i] == 5 and len(temp_sent) > 0:\n",
    "                dis_list.append(temp_sent)\n",
    "                temp_sent = vocab.IdToPiece(int(token_ids[0][i]))\n",
    "            else:\n",
    "                temp_sent += vocab.IdToPiece(int(token_ids[0][i]))\n",
    "        elif len(temp_sent) > 0:\n",
    "            dis_list.append(temp_sent)\n",
    "            temp_sent = ''\n",
    "print(dis_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_make(model2, input_text):\n",
    "    \n",
    "    # model\n",
    "    model = model2\n",
    "    model.eval()\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    vocab_file = \"vocab.model\"\n",
    "    vocab = spm.SentencePieceProcessor()\n",
    "    vocab.load(vocab_file)\n",
    "    model.to(device)\n",
    "    text_sent = []\n",
    "    text_sent.append(input_text)\n",
    "    sent_test = BERTTESTsent(text_sent, tok, 512, True, False)\n",
    "    sent_dataloader = torch.utils.data.DataLoader(sent_test, batch_size=1, num_workers = 5)\n",
    "    temp_sent = ''\n",
    "    dis_list = []\n",
    "\n",
    "    for _, (token_ids, valid_length, segment_ids) in enumerate(sent_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids =  segment_ids.long().to(device)\n",
    "        valid_length =  valid_length\n",
    "        sequence_of_tags = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "        for i in range(int(valid_length)):\n",
    "            if sequence_of_tags[0][i] == 5 or sequence_of_tags[0][i] == 6:\n",
    "                if sequence_of_tags[0][i] == 5 and len(temp_sent) > 0:\n",
    "                    dis_list.append(temp_sent)\n",
    "                    temp_sent = vocab.IdToPiece(int(token_ids[0][i]))\n",
    "                else:\n",
    "                    temp_sent += vocab.IdToPiece(int(token_ids[0][i]))\n",
    "            elif len(temp_sent) > 0:\n",
    "                dis_list.append(temp_sent)\n",
    "                temp_sent = ''\n",
    "    return dis_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequence_of_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb273110e664e798b8010c9c81338c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13995.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fd7a863eb70>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/RNTier/home/halee/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/RNTier/home/halee/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1174, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tqdm\n",
    "\n",
    "# anago model load\n",
    "anago_pet_ner_model = torch.load('KoBERT_DIS_NER_V3/Human_Pet_DIS_V3(epoch3, step300).pth')\n",
    "# QA Data load\n",
    "with open(\"Data/Vet_Data_Total_Not_Null_Question.json\", \"r\", encoding='utf-8') as pet_data_json:\n",
    "    pet_data = json.load(pet_data_json)\n",
    "json_file_resultList = []\n",
    "cnt = 0\n",
    "for i in tqdm_notebook(pet_data):\n",
    "    title = i['title']\n",
    "    question = i['question']\n",
    "    answer = i['answer']\n",
    "    url = i['url']\n",
    "    answer_test = answer.split('.')\n",
    "    if (answer_test[-1] == '') or answer_test[-1] == ' ':\n",
    "        del (answer_test[-1])\n",
    "    answer_test = [i + \".\" for i in answer_test]\n",
    "    answer_ner = []  # 개체명 인식 결과 저장 리스트\n",
    "    sentence_info = []  # 개체명 인식 결과 문장 리스트\n",
    "    for sentence in answer_test:\n",
    "        sentence_tmp = sentence\n",
    "        sentence = sentence.replace(' ', '')\n",
    "        a = ner_make(anago_pet_ner_model, sentence)\n",
    "        \n",
    "        if len(a) > 0:\n",
    "            for tmp in a:\n",
    "                answer_ner.append(tmp)\n",
    "            sentence_info.append(sentence_tmp)\n",
    "    # 결과 확인 및 개체명 생성 완료된 json file 생성\n",
    "    # set -> list : 중복제거\n",
    "    answer_ner = set(answer_ner)\n",
    "    answer_ner = list(answer_ner)\n",
    "    # set -> list : 중복제거\n",
    "    sentence_info = set(sentence_info)\n",
    "    sentence_info = list(sentence_info)\n",
    "    result = {\n",
    "        \"title\": title,\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"answer_ner\": str(answer_ner),\n",
    "        \"sentence_info\": str(sentence_info),\n",
    "        \"url\": url\n",
    "    }\n",
    "    json_file_resultList.append(result)\n",
    "    if cnt % 100 == 0:\n",
    "        print(cnt)\n",
    "    cnt+=1\n",
    "# # json 파일 저장\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "        'Data/Vet_Data_Total_Not_Null_Question_NER.json',\n",
    "        'w', encoding=\"utf-8\") as make_file:\n",
    "    json.dump(json_file_resultList, make_file, ensure_ascii=False, indent=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7999ffa372d5518938264c39b302fc0f2ea2294124935d58b3105141df28eea9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
