{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 필요한 라이브러리 Import"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "from KoBERT.kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from torch import nn\n",
    "from transformers import BertModel, BertConfig\n",
    "from torchcrf import CRF\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
    "from gluonnlp.data import BERTSPTokenizer\n",
    "import sentencepiece as spm\n",
    "from seqeval.metrics import f1_score\n",
    "from pathlib import Path\n",
    "import json"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Config:\n",
    "    def __init__(self, json_path):\n",
    "        with open(json_path, mode='r') as io:\n",
    "            params = json.loads(io.read())\n",
    "        self.__dict__.update(params)\n",
    "\n",
    "    def save(self, json_path):\n",
    "        with open(json_path, mode='w') as io:\n",
    "            json.dump(self.__dict__, io, indent=4)\n",
    "\n",
    "    def update(self, json_path):\n",
    "        with open(json_path, mode='r') as io:\n",
    "            params = json.loads(io.read())\n",
    "        self.__dict__.update(params)\n",
    "\n",
    "    @property\n",
    "    def dict(self):\n",
    "        return self.__dict__"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# KoBERT + CRF"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#클래스 참고: KoBERT와 CRF로 만든 한국어 개체명 인식 https://github.com/eagle705/pytorch-bert-crf-ner\n",
    "\n",
    "bert_config = {'attention_probs_dropout_prob': 0.1,\n",
    "                 'hidden_act': 'gelu',\n",
    "                 'hidden_dropout_prob': 0.1,\n",
    "                 'hidden_size': 768,\n",
    "                 'initializer_range': 0.02,\n",
    "                 'intermediate_size': 3072,\n",
    "                 'max_position_embeddings': 512,\n",
    "                 'num_attention_heads': 12,\n",
    "                 'num_hidden_layers': 12,\n",
    "                 'type_vocab_size': 2,\n",
    "                 'vocab_size': 3517,\n",
    "                 'padding': True,\n",
    "                 'pair': False\n",
    "                 }\n",
    "                 \n",
    "class KobertCRF(nn.Module):\n",
    "    \"\"\" KoBERT with CRF \"\"\"\n",
    "    def __init__(self, config, num_classes, vocab=None) -> None:\n",
    "        super(KobertCRF, self).__init__()\n",
    "\n",
    "        if vocab is None:\n",
    "            self.bert, self.vocab = get_pytorch_kobert_model()\n",
    "        else:\n",
    "            self.bert = BertModel(config=BertConfig.from_dict(bert_config))\n",
    "            self.vocab = vocab\n",
    "\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.position_wise_ff = nn.Linear(config.hidden_size, num_classes)\n",
    "        self.crf = CRF(num_tags=num_classes, batch_first=True)\n",
    "\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "        \n",
    "    def forward(self, input_ids, valid_length, token_type_ids=None, tags=None):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "\n",
    "        # outputs: (last_encoder_layer, pooled_output, attention_weight)\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            attention_mask=attention_mask.float().to(token_ids.device))\n",
    "        last_encoder_layer = outputs[0]\n",
    "        last_encoder_layer = self.dropout(last_encoder_layer)\n",
    "        emissions = self.position_wise_ff(last_encoder_layer)\n",
    "\n",
    "        if tags is not None:\n",
    "            log_likelihood, sequence_of_tags = self.crf(emissions, tags), self.crf.decode(emissions)\n",
    "            return log_likelihood, sequence_of_tags\n",
    "        else:\n",
    "            sequence_of_tags = self.crf.decode(emissions)\n",
    "            return sequence_of_tags"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenizer & Vocab 생성\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Tokenizer model 생성\n",
    "\n",
    "corpus = \"vocab.txt\" # KoCharELECTRA (https://github.com/monologg/KoCharELECTRA) vocab.txt 파일 사용 (음절 vocab 파일)\n",
    "prefix = \"vocab\"\n",
    "vocab_size = 3511 # vocab.txt 크기\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={corpus} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
    "    \" --model_type=bpe\" +\n",
    "    \" --max_sentence_length=100\" + # 문장 최대 길이\n",
    "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
    "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
    "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
    "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
    "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bertmodel, _ = get_pytorch_kobert_model() # KoBERT 모델 불러오기\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece('vocab.model', padding_token='[PAD]')\n",
    "tokenizer = \"vocab.model\" #생성한 vocab.model 이용\n",
    "tok = BERTSPTokenizer(tokenizer, vocab, lower=False) # BERTSPTokenizer 생성"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 데이터 셋 불러오기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "train_list_csv = pd.read_csv('Data/pet_train.csv', converters={\"0\": literal_eval})\n",
    "test_list_csv = pd.read_csv('Data/pet_test.csv', converters={\"0\": literal_eval})\n",
    "tr_tag = train_list_csv['0']\n",
    "tr_sent = train_list_csv['1']\n",
    "ts_tag = test_list_csv['0']\n",
    "ts_sent = test_list_csv['1']\n",
    "\n",
    "del_index = 0\n",
    "\n",
    "for i in range(len(train_list_csv[\"0\"])):\n",
    "    if len(tr_tag[i]) != len(tr_sent[i])+3: #데이터 셋 준비 과정 중 길이 다르면 해당 인댁스 출력 +3 해주는 이유는 버트에 넣기 전 붙을 태그 등 데이터 떄문\n",
    "        print(i)\n",
    "    if len(tr_tag[i]) > 512 or len(tr_sent[i])+3 > 512: # 문장의 길이가 512가 넘는 문장 제거\n",
    "        del tr_tag[i]\n",
    "for i in range(len(test_list_csv[\"0\"])):\n",
    "    if len(ts_tag[i]) != len(ts_sent[i])+3:\n",
    "        print(i)\n",
    "    if len(ts_tag[i]) > 512 or len(ts_sent[i])+3 > 512: # 문장의 길이가 512가 넘는 문장 제거\n",
    "        del tr_sent[i]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 버트 데이터 셋 클래스 생성"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 코드 참고: 버트를 이용한 네이버 댓글 분류기 https://github.com/SKTBrain/KoBERT/blob/master/scripts/NSMC/naver_review_classifications_pytorch_kobert.ipynb\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, sent, tag, bert_tokenizer, max_len,\n",
    "                pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_len, pad=pad, pair=pair)\n",
    "        self.sentences = [transform([i]) for i in sent] #문장\n",
    "        self.labels = [np.pad(np.int32(i), (0,max_len-len(i)), 'constant', constant_values=0) for i in tag] #질병 태그 및 질병 태그 max_len 패딩\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 사용자 입력 문장을 테스하기 위한 데이터 셋 클래스 생성"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class BERTTESTsent(Dataset):\n",
    "    def __init__(self, sent, bert_tokenizer, max_len,\n",
    "                pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_len, pad=pad, pair=pair)\n",
    "        self.sentences = [transform([i]) for i in sent] #문장\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i])\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.sentences))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 데이터 셋 나누기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_train = BERTDataset(tr_sent,tr_tag, tok, 512, True, False)\n",
    "data_test = BERTDataset(ts_sent,ts_tag, tok, 512, True, False)\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=3, num_workers = 5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=1, num_workers = 5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 모델 선언 (초기 학습을 위한)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device('cuda')\n",
    "model_config = Config('config.json')\n",
    "model = KobertCRF(config=model_config, num_classes=7)\n",
    "model.to(device)\n",
    "model.train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 학습된 모델 불러오기 (파인 튜닝을 위한)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = torch.load('KoBERT_DIS_NER/Human_PET_DIS2.pth')\n",
    "model_config = Config('config.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 모델 환경 설정"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def set_seed(seed=100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(\n",
    "        nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "\n",
    "t_total = len(\n",
    "    train_dataloader) // model_config.gradient_accumulation_steps * model_config.epochs\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                    lr=model_config.learning_rate, eps=model_config.adam_epsilon)\n",
    "scheduler = WarmupLinearSchedule(\n",
    "    optimizer, warmup_steps=model_config.warmup_steps, t_total=t_total)\n",
    "\n",
    "model_dir = 'DIS_NER/'\n",
    "\n",
    "global_step = 0\n",
    "tr_loss, logging_loss = 0.0, 0.0\n",
    "model.zero_grad()\n",
    "set_seed()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 모델 학습"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "train_iterator = trange(int(model_config.epochs), desc=\"Epoch\")\n",
    "device = torch.device('cuda')\n",
    "for _epoch, _ in enumerate(train_iterator):\n",
    "    epoch = _epoch\n",
    "    for step, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        \n",
    "        model.train()\n",
    " \n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        log_likelihood, sequence_of_tags = model(token_ids,valid_length, segment_ids, label)\n",
    "\n",
    "        # loss: negative log-likelihood\n",
    "        loss = -1 * log_likelihood\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), model_config.max_grad_norm)\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % model_config.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            with torch.no_grad():\n",
    "                sequence_of_tags = torch.tensor(\n",
    "                    sequence_of_tags).to(device)\n",
    "                mb_acc = (sequence_of_tags == label).float()[label != 0].mean()\n",
    "\n",
    "            tr_acc = mb_acc.item()\n",
    "            tr_loss_avg = tr_loss / global_step\n",
    "            tr_summary = {'loss': tr_loss_avg, 'acc': tr_acc}\n",
    "            state = {'global_step': global_step + 1,\n",
    "                             'model_state_dict': model.state_dict(),\n",
    "                             'opt_state_dict': optimizer.state_dict()}\n",
    "            if step % 100 == 0:\n",
    "                print('epoch : {}, global_step : {}, tr_loss: {:.3f}, tr_acc: {:.2%}'.format(\n",
    "                        epoch + 1, global_step, tr_summary['loss'], tr_summary['acc']))\n",
    "            "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 모델 저장"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.save(model, 'KoBERT_DIS_NER/Human_DIS.pth')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 모델 평가"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def index_to_ner(sequences, val_len):\n",
    "\n",
    "    idx_to_ner = {\n",
    "        1 : \"[CLS]\", \n",
    "        2 : \"[SEP]\", \n",
    "        0 : \"O\", #[PAD] to 'O'\n",
    "        3 : \"[MASK]\",\n",
    "        4 : \"O\",\n",
    "        5 : \"B-DIS\",\n",
    "        6 : \"I-DIS\"\n",
    "    }\n",
    "    result = []\n",
    "    for i in range(val_len):\n",
    "        result.append(idx_to_ner[sequences[i]])\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device('cuda')\n",
    "model_config = Config('config.json')\n",
    "model = torch.load('KoBERT_DIS_NER/Human_PET_DIS2.pth')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "eval_step = 0\n",
    "sum_f1 = 0\n",
    "predic_list = []\n",
    "true_list = []\n",
    "predic_list2= []\n",
    "true_list2 = []\n",
    "for step, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "    \n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    label = label.long().to(device)\n",
    "\n",
    "    log_likelihood, sequence_of_tags = model(token_ids,valid_length, segment_ids, label)\n",
    "    # loss: negative log-likelihood\n",
    "    eval_loss = -1 * log_likelihood\n",
    "\n",
    "    if (eval_step + 1) % model_config.gradient_accumulation_steps == 0:\n",
    "        eval_step += 1\n",
    "        with torch.no_grad():\n",
    "            sequence_of_tags = torch.tensor(\n",
    "                sequence_of_tags).to(device)\n",
    "            eval_acc = (sequence_of_tags == label).float()[label != 0].mean()\n",
    "        eval_loss_avg = eval_loss / eval_step\n",
    "        eval_summary = {'loss': eval_loss_avg, 'acc': eval_acc}\n",
    "\n",
    "        predic_list.append(index_to_ner(sequence_of_tags[0].cpu().numpy(), valid_length[0].cpu().numpy()))\n",
    "        true_list.append(index_to_ner(label[0].cpu().numpy(), valid_length[0].cpu().numpy()))\n",
    "        \n",
    "        if eval_step % 10 == 0:\n",
    "            print('eval_step : {}, evl_loss: {:.3f}, evl_acc: {:.2f}'.format(\n",
    "                eval_step, eval_summary['loss'], eval_summary['acc']))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## F1 스코어 평가"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f1_score(true_list, predic_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 사용자 입력 문장 모델 평가"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bertmodel, _ = get_pytorch_kobert_model() # KoBERT 모델 불러오기\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece('vocab.model', padding_token='[PAD]')\n",
    "tokenizer = \"vocab.model\" #생성한 vocab.model 이용\n",
    "tok = BERTSPTokenizer(tokenizer, vocab, lower=False) # BERTSPTokenizer 생성"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_config = Config('config.json')\n",
    "\n",
    "# model\n",
    "model = torch.load('KoBERT_DIS_NER/Human_PET_DIS.pth')\n",
    "model.eval()\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "input_text = input('input> ')\n",
    "print(input_text)\n",
    "text_sent = []\n",
    "text_sent.append(input_text)\n",
    "sent_test = BERTTESTsent(text_sent, tok, 512, True, False)\n",
    "sent_dataloader = torch.utils.data.DataLoader(sent_test, batch_size=1, num_workers = 5)\n",
    "\n",
    "for _, (token_ids, valid_length, segment_ids) in enumerate(tqdm_notebook(sent_dataloader)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids =  segment_ids.long().to(device)\n",
    "    valid_length =  valid_length\n",
    "    sequence_of_tags = model(token_ids, valid_length, segment_ids)\n",
    "    print(sequence_of_tags)\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('summer_sig': conda)"
  },
  "interpreter": {
   "hash": "7999ffa372d5518938264c39b302fc0f2ea2294124935d58b3105141df28eea9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}